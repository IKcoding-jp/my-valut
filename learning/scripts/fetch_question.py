#!/usr/bin/env python3
"""
éå»å•è‡ªå‹•å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
fe-siken.com ã‹ã‚‰åŸºæœ¬æƒ…å ±æŠ€è¡“è€…è©¦é¨“ã®å•é¡Œã‚’å–å¾—ã—ã€
æ—¢å­˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ã¦ä¿å­˜ã™ã‚‹ã€‚

å¯¾å¿œå¹´åº¦: H21æ˜¥ã€œR06ï¼ˆæ–°åˆ¶åº¦ãƒ»æ—§åˆ¶åº¦ä¸¡å¯¾å¿œï¼‰
- æ–°åˆ¶åº¦ï¼ˆR02ã€œR06ï¼‰: ç§‘ç›®A 60å•ï¼ˆallæŒ‡å®šæ™‚ï¼‰
- æ—§åˆ¶åº¦ï¼ˆã€œR01ï¼‰: ç§‘ç›®A 80å•ï¼ˆallæŒ‡å®šæ™‚ï¼‰
- MAX_QUESTIONS_PER_RUN: 80ï¼ˆ1å›ã®å®Ÿè¡Œã§æœ€å¤§80å•ã¾ã§å–å¾—å¯èƒ½ï¼‰

ç”»åƒURL: ãƒšãƒ¼ã‚¸URLã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ï¼ˆbase_urlï¼‰ã‚’ä½¿ã„ã€
ç›¸å¯¾ãƒ‘ã‚¹ã®ç”»åƒã‚’çµ¶å¯¾URLã«è‡ªå‹•å¤‰æ›ã™ã‚‹ã€‚

ä½¿ã„æ–¹:
    # å˜ä¸€å•é¡Œå–å¾—
    python scripts/fetch_question.py R05 15
    python scripts/fetch_question.py H30æ˜¥ 15

    # ç¯„å›²æŒ‡å®šå–å¾—
    python scripts/fetch_question.py R05 1-20

    # ç§‘ç›®Aå…¨å•å–å¾—ï¼ˆR02ä»¥é™: 60å•ã€R01ä»¥å‰: 80å•ï¼‰
    python scripts/fetch_question.py R05 all
"""

import sys
import os
import re
import time
import json
import io
import requests
from bs4 import BeautifulSoup
from pathlib import Path

# Windowsç’°å¢ƒã§ã®UTF-8å‡ºåŠ›ã‚’å¼·åˆ¶ï¼ˆçµµæ–‡å­—ãƒ»æ—¥æœ¬èªã®æ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
if sys.stdout.encoding != "utf-8":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
if sys.stderr.encoding != "utf-8":
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8", errors="replace")

# â”€â”€ è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BASE_URL = "https://www.fe-siken.com/kakomon"
REPO_ROOT = Path(__file__).resolve().parent.parent
QUESTIONS_DIR = REPO_ROOT / "questions" / "kamoku_a"
PROGRESS_FILE = REPO_ROOT / "learning" / "é€²æ—ãƒ­ã‚°.md"
REQUEST_INTERVAL = 2.5  # ã‚µãƒ¼ãƒãƒ¼è² è·å¯¾ç­–ï¼ˆç§’ï¼‰
MAX_QUESTIONS_PER_RUN = 80  # æ—§åˆ¶åº¦80å•ã«ã‚‚å¯¾å¿œ
USER_AGENT = "Mozilla/5.0 (compatible; FEvault-study-tool/1.0)"

# â”€â”€ å¹´åº¦ã‚³ãƒ¼ãƒ‰ â†’ URLãƒ‘ã‚¹å¤‰æ› â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# æ–°åˆ¶åº¦ï¼ˆä»¤å’Œ5å¹´ã€œï¼‰: XX_menjo
# æ—§åˆ¶åº¦ï¼ˆã€œä»¤å’Œ4å¹´ï¼‰: XX_haru / XX_aki
# ä»¤å’Œ2-4å¹´: XX_menjo (å…é™¤è©¦é¨“å½¢å¼)
YEAR_MAP = {
    # æ–°åˆ¶åº¦ CBTï¼ˆä»¤å’Œ5å¹´ã€œï¼‰
    "R07": "07_menjo", "R06": "06_menjo", "R05": "05_menjo",
    # æ—§åˆ¶åº¦ â†’ å…é™¤è©¦é¨“
    "R04": "04_menjo", "R03": "03_menjo", "R02": "02_menjo",
    # æ—§åˆ¶åº¦ï¼ˆæ˜¥ç§‹ã‚ã‚Šï¼‰
    "R01ç§‹": "01_aki", "R01æ˜¥": "31_haru",
    "H31æ˜¥": "31_haru",
    "H30ç§‹": "30_aki", "H30æ˜¥": "30_haru",
    "H29ç§‹": "29_aki", "H29æ˜¥": "29_haru",
    "H28ç§‹": "28_aki", "H28æ˜¥": "28_haru",
    "H27ç§‹": "27_aki", "H27æ˜¥": "27_haru",
    "H26ç§‹": "26_aki", "H26æ˜¥": "26_haru",
    "H25ç§‹": "25_aki", "H25æ˜¥": "25_haru",
    "H24ç§‹": "24_aki", "H24æ˜¥": "24_haru",
    "H23ç§‹": "23_aki", "H23ç‰¹": "23_toku",
    "H22ç§‹": "22_aki", "H22æ˜¥": "22_haru",
    "H21ç§‹": "21_aki", "H21æ˜¥": "21_haru",
}

# å¹´åº¦ã‚³ãƒ¼ãƒ‰ â†’ è¥¿æš¦å¹´ï¼ˆfrontmatterç”¨ï¼‰
YEAR_TO_WESTERN = {
    "R07": 2025, "R06": 2024, "R05": 2023, "R04": 2022,
    "R03": 2021, "R02": 2020, "R01": 2019,
    "H31": 2019, "H30": 2018, "H29": 2017, "H28": 2016,
    "H27": 2015, "H26": 2014, "H25": 2013, "H24": 2012,
    "H23": 2011, "H22": 2010, "H21": 2009,
}

# å¹´åº¦ã‚³ãƒ¼ãƒ‰ â†’ æ—¥æœ¬èªè¡¨è¨˜
YEAR_TO_LABEL = {
    "R07": "ä»¤å’Œ7å¹´åº¦", "R06": "ä»¤å’Œ6å¹´åº¦", "R05": "ä»¤å’Œ5å¹´åº¦",
    "R04": "ä»¤å’Œ4å¹´åº¦", "R03": "ä»¤å’Œ3å¹´åº¦", "R02": "ä»¤å’Œ2å¹´åº¦",
    "R01ç§‹": "ä»¤å’Œå…ƒå¹´ç§‹æœŸ", "R01æ˜¥": "ä»¤å’Œå…ƒå¹´æ˜¥æœŸ",
    "H31æ˜¥": "å¹³æˆ31å¹´æ˜¥æœŸ",
    "H30ç§‹": "å¹³æˆ30å¹´ç§‹æœŸ", "H30æ˜¥": "å¹³æˆ30å¹´æ˜¥æœŸ",
    "H29ç§‹": "å¹³æˆ29å¹´ç§‹æœŸ", "H29æ˜¥": "å¹³æˆ29å¹´æ˜¥æœŸ",
    "H28ç§‹": "å¹³æˆ28å¹´ç§‹æœŸ", "H28æ˜¥": "å¹³æˆ28å¹´æ˜¥æœŸ",
    "H27ç§‹": "å¹³æˆ27å¹´ç§‹æœŸ", "H27æ˜¥": "å¹³æˆ27å¹´æ˜¥æœŸ",
    "H26ç§‹": "å¹³æˆ26å¹´ç§‹æœŸ", "H26æ˜¥": "å¹³æˆ26å¹´æ˜¥æœŸ",
    "H25ç§‹": "å¹³æˆ25å¹´ç§‹æœŸ", "H25æ˜¥": "å¹³æˆ25å¹´æ˜¥æœŸ",
    "H24ç§‹": "å¹³æˆ24å¹´ç§‹æœŸ", "H24æ˜¥": "å¹³æˆ24å¹´æ˜¥æœŸ",
    "H23ç§‹": "å¹³æˆ23å¹´ç§‹æœŸ", "H23ç‰¹": "å¹³æˆ23å¹´ç‰¹åˆ¥",
    "H22ç§‹": "å¹³æˆ22å¹´ç§‹æœŸ", "H22æ˜¥": "å¹³æˆ22å¹´æ˜¥æœŸ",
    "H21ç§‹": "å¹³æˆ21å¹´ç§‹æœŸ", "H21æ˜¥": "å¹³æˆ21å¹´æ˜¥æœŸ",
}

# â”€â”€ åˆ†é‡åˆ¤å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# fe-siken.comã®åˆ†é¡ â†’ ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
FIELD_TO_DIR = {
    "åŸºç¤ç†è«–": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/åŸºç¤ç†è«–",
    "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/åŸºç¤ç†è«–",
    "é›¢æ•£æ•°å­¦": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/åŸºç¤ç†è«–",
    "å¿œç”¨æ•°å­¦": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/åŸºç¤ç†è«–",
    "æƒ…å ±ã«é–¢ã™ã‚‹ç†è«–": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/åŸºç¤ç†è«–",
    "é€šä¿¡ã«é–¢ã™ã‚‹ç†è«–": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/åŸºç¤ç†è«–",
    "è¨ˆæ¸¬ãƒ»åˆ¶å¾¡ã«é–¢ã™ã‚‹ç†è«–": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/åŸºç¤ç†è«–",
    "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿æ§‹æˆè¦ç´ ": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆè¦ç´ ": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "ãƒ—ãƒ­ã‚»ãƒƒã‚µ": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "ãƒ¡ãƒ¢ãƒª": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "å…¥å‡ºåŠ›ãƒ‡ãƒã‚¤ã‚¹": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "ãƒã‚¹": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "é–‹ç™ºãƒ„ãƒ¼ãƒ«": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡æŒ‡æ¨™": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚·ã‚¹ãƒ†ãƒ ",
    "ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "ãƒ‡ãƒ¼ã‚¿æ“ä½œ": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç†": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ–¹å¼": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "é€šä¿¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¿œç”¨": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "æƒ…å ±ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "æƒ…å ±ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç®¡ç†": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "æƒ…å ±ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…æŠ€è¡“": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ ",
    "ã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºæŠ€è¡“": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/é–‹ç™ºæŠ€è¡“",
    "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºç®¡ç†æŠ€è¡“": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/é–‹ç™ºæŠ€è¡“",
    "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ": "ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆç³»",
    "ã‚µãƒ¼ãƒ“ã‚¹ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ": "ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆç³»",
    "ã‚·ã‚¹ãƒ†ãƒ ç›£æŸ»": "ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆç³»",
    "ã‚·ã‚¹ãƒ†ãƒ æˆ¦ç•¥": "ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ç³»",
    "ã‚·ã‚¹ãƒ†ãƒ ä¼ç”»": "ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ç³»",
    "çµŒå–¶æˆ¦ç•¥ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ": "ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ç³»",
    "æŠ€è¡“æˆ¦ç•¥ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ": "ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ç³»",
    "ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ€ã‚¹ãƒˆãƒª": "ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ç³»",
    "ä¼æ¥­æ´»å‹•": "ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ç³»",
    "æ³•å‹™": "ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ç³»",
    "ä¼šè¨ˆãƒ»è²¡å‹™": "ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ç³»",
}

# åˆ†é‡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â†’ ç´°åˆ†é¡ï¼ˆfield ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ç”¨ï¼‰
FIELD_KEYWORDS = {
    "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£": ["æš—å·", "èªè¨¼", "æ”»æ’ƒ", "ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«", "ãƒãƒ«ã‚¦ã‚§ã‚¢", "è„†å¼±æ€§",
                   "SSL", "TLS", "PKI", "VPN", "WAF", "IDS", "IPS", "ãƒ‡ã‚£ã‚¸ã‚¿ãƒ«ç½²å",
                   "å…¬é–‹éµ", "ç§˜å¯†éµ", "å…±é€šéµ", "ãƒãƒƒã‚·ãƒ¥", "HTTPS"],
    "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯": ["IP", "ãƒ«ãƒ¼ã‚¿", "ãƒ—ãƒ­ãƒˆã‚³ãƒ«", "TCP", "UDP", "DNS", "DHCP",
                   "ã‚µãƒ–ãƒãƒƒãƒˆ", "MACã‚¢ãƒ‰ãƒ¬ã‚¹", "LAN", "WAN", "OSI", "HTTP",
                   "SMTP", "POP", "IMAP", "FTP", "NTP", "ãƒãƒ¼ãƒˆç•ªå·"],
    "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹": ["SQL", "æ­£è¦åŒ–", "ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³", "ACID", "ãƒ­ãƒƒã‚¯", "ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯",
                   "ä¸»ã‚­ãƒ¼", "å¤–éƒ¨ã‚­ãƒ¼", "SELECT", "INSERT", "é–¢ä¿‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ERå›³",
                   "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹", "ãƒ“ãƒ¥ãƒ¼"],
    "ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ": ["ç¨¼åƒç‡", "MTBF", "MTTR", "ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ", "ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ",
                   "ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ãƒˆ", "å†—é•·åŒ–", "è² è·åˆ†æ•£", "ã‚¯ãƒ©ã‚¹ã‚¿",
                   "ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ", "ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—"],
}

# â”€â”€ å•é¡Œã‚¿ã‚¤ãƒ—è‡ªå‹•åˆ¤å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def classify_question_type(question_text, explanation_text, field_path):
    """å•é¡Œã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤å®šã™ã‚‹ï¼ˆæš—è¨˜/ç†è§£/æ··åˆï¼‰"""
    text = question_text + explanation_text

    # è¨ˆç®—ãƒ»ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    understanding_keywords = [
        "è¨ˆç®—", "æ±‚ã‚ã‚ˆ", "ã„ãã¤ã‹", "ä½•å€", "ä½•ãƒ“ãƒƒãƒˆ", "ç¨¼åƒç‡",
        "ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ", "ã‚¢ãƒ‰ãƒ¬ã‚¹", "å¤‰æ›", "è«–ç†å¼", "çœŸç†å€¤",
        "é€²æ•°", "åŸºæ•°", "è£œæ•°", "æµ®å‹•å°æ•°ç‚¹", "ã‚½ãƒ¼ãƒˆ", "æ¢ç´¢",
        "ãƒˆãƒ¬ãƒ¼ã‚¹", "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", "ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ",
    ]

    # æš—è¨˜ä¸­å¿ƒã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    memorization_keywords = [
        "ã©ã‚Œã‹", "é©åˆ‡ãªã‚‚ã®", "æ­£ã—ã„ã‚‚ã®", "è¨˜è¿°ã¨ã—ã¦",
        "èª¬æ˜ã¨ã—ã¦", "ç‰¹å¾´ã¨ã—ã¦", "ç›®çš„ã¨ã—ã¦",
    ]

    has_understanding = any(kw in text for kw in understanding_keywords)
    has_memorization = any(kw in text for kw in memorization_keywords)

    if has_understanding and has_memorization:
        return "æ··åˆ"
    elif has_understanding:
        return "ç†è§£"
    else:
        return "æš—è¨˜"


# â”€â”€ HTML ãƒ‘ãƒ¼ã‚µãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_question_page(url):
    """æŒ‡å®šURLã‹ã‚‰HTMLã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹ã™ã‚‹"""
    headers = {"User-Agent": USER_AGENT}
    response = requests.get(url, headers=headers, timeout=15)
    response.encoding = "utf-8"

    if response.status_code != 200:
        return None, f"HTTP {response.status_code}"

    return BeautifulSoup(response.text, "html.parser"), None


def parse_question(soup, url):
    """BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰å•é¡Œãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹"""
    data = {"source_url": url}

    # ãƒšãƒ¼ã‚¸URLã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ã‚’å–å¾—ï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã®ç”»åƒURLè§£æ±ºç”¨ï¼‰
    base_url = url.rsplit("/", 1)[0] + "/"  # ä¾‹: https://www.fe-siken.com/kakomon/05_menjo/

    # å•é¡Œæ–‡
    mondai_div = soup.find("div", id="mondai")
    if mondai_div:
        # ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®ä¸¡æ–¹ã‚’å–å¾—
        data["question_text"] = get_content_with_images(mondai_div, base_url)
    else:
        return None, "å•é¡Œæ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

    # é¸æŠè‚¢
    choices = {}
    for label, sel_id in [("ã‚¢", "select_a"), ("ã‚¤", "select_i"),
                           ("ã‚¦", "select_u"), ("ã‚¨", "select_e")]:
        span = soup.find("span", id=sel_id)
        if span:
            choices[label] = get_content_with_images(span, base_url)
    data["choices"] = choices

    # æ­£è§£
    answer_box = soup.find("div", class_="answerBox")
    if answer_box:
        answer_text = answer_box.get_text(strip=True)
        # ã€Œæ­£è§£ã‚’è¡¨ç¤ºã™ã‚‹ã€ãƒœã‚¿ãƒ³ã®å¾Œã®æ–‡å­—ã‚’å–å¾—
        answer_text = answer_text.replace("æ­£è§£ã‚’è¡¨ç¤ºã™ã‚‹", "").strip()
        if answer_text:
            data["answer"] = answer_text[0]  # ã‚¢/ã‚¤/ã‚¦/ã‚¨ã®1æ–‡å­—
        else:
            # ãƒœã‚¿ãƒ³å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã¯å–ã‚Œãªã„å ´åˆã€kaisetsu ã‹ã‚‰æ¨å®š
            data["answer"] = guess_answer_from_explanation(soup)
    else:
        data["answer"] = guess_answer_from_explanation(soup)

    # è§£èª¬
    kaisetsu_div = soup.find("div", id="kaisetsu")
    if kaisetsu_div:
        explanation_parts = []
        for li in kaisetsu_div.find_all("li"):
            cls = li.get("class", [])
            label = ""
            if "lia" in cls:
                label = "ã‚¢"
            elif "lii" in cls:
                label = "ã‚¤"
            elif "liu" in cls:
                label = "ã‚¦"
            elif "lie" in cls:
                label = "ã‚¨"
            text = li.get_text(strip=True)
            if label:
                explanation_parts.append(f"- {label}: {text}")
            else:
                explanation_parts.append(text)

        # è§£èª¬ã®ãƒªã‚¹ãƒˆä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚å–å¾—
        general_text_parts = []
        for child in kaisetsu_div.children:
            if hasattr(child, "name"):
                if child.name not in ["ul", "ol", "script", "ins"]:
                    t = child.get_text(strip=True)
                    if t:
                        general_text_parts.append(t)
            elif isinstance(child, str) and child.strip():
                general_text_parts.append(child.strip())

        data["explanation_general"] = "\n".join(general_text_parts)
        data["explanation_choices"] = "\n".join(explanation_parts)
    else:
        data["explanation_general"] = ""
        data["explanation_choices"] = ""

    # åˆ†é¡
    field_raw = ""
    for h3 in soup.find_all("h3"):
        if "åˆ†é¡" in h3.get_text():
            next_div = h3.find_next_sibling("div")
            if next_div:
                field_raw = next_div.get_text(strip=True)
                break
    data["field_raw"] = field_raw

    # å‡ºå…¸æƒ…å ±
    source_list = soup.find_all("ul", class_="recentList")
    if source_list:
        data["source_info"] = source_list[0].get_text(strip=True)
    else:
        data["source_info"] = ""

    return data, None


def get_content_with_images(element, base_url=""):
    """ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’å«ã‚€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹

    base_url: ãƒšãƒ¼ã‚¸URLã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªéƒ¨åˆ†ï¼ˆä¾‹: https://www.fe-siken.com/kakomon/05_menjo/ï¼‰
              ç›¸å¯¾ãƒ‘ã‚¹ã®ç”»åƒã‚’æ­£ã—ãè§£æ±ºã™ã‚‹ãŸã‚ã«ä½¿ç”¨
    """
    parts = []
    for child in element.descendants:
        if isinstance(child, str):
            text = child.strip()
            if text:
                parts.append(text)
        elif hasattr(child, "name") and child.name == "img":
            src = child.get("src", "")
            alt = child.get("alt", "ç”»åƒ")
            if src and not src.startswith("http"):
                if src.startswith("/"):
                    # çµ¶å¯¾ãƒ‘ã‚¹ï¼ˆ/img/titlelogo.pngç­‰ï¼‰
                    src = f"https://www.fe-siken.com{src}"
                elif base_url:
                    # ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆimg/55.pngç­‰ï¼‰â†’ ãƒšãƒ¼ã‚¸URLãƒ™ãƒ¼ã‚¹ã§è§£æ±º
                    src = f"{base_url}{src}"
                else:
                    src = f"https://www.fe-siken.com/{src}"
            parts.append(f"![{alt}]({src})")
        elif hasattr(child, "name") and child.name == "br":
            parts.append("\n")
    return "".join(parts)


def guess_answer_from_explanation(soup):
    """è§£èª¬ã‹ã‚‰æ­£è§£ã‚’æ¨å®šã™ã‚‹"""
    kaisetsu = soup.find("div", id="kaisetsu")
    if not kaisetsu:
        return "?"
    for li in kaisetsu.find_all("li"):
        text = li.get_text(strip=True)
        if "æ­£ã—ã„" in text or "æ­£è§£" in text or "â—‹" in text:
            cls = li.get("class", [])
            if "lia" in cls:
                return "ã‚¢"
            elif "lii" in cls:
                return "ã‚¤"
            elif "liu" in cls:
                return "ã‚¦"
            elif "lie" in cls:
                return "ã‚¨"
    return "?"


# â”€â”€ åˆ†é‡ã®è§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_field(field_raw):
    """fe-siken.comã®åˆ†é¡æ–‡å­—åˆ—ã‹ã‚‰ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’ç”Ÿæˆã™ã‚‹"""
    # ä¾‹: "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³» Â» ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ Â» é–‹ç™ºãƒ„ãƒ¼ãƒ«"
    parts = [p.strip() for p in field_raw.split("Â»")]

    # æœ€ã‚‚å…·ä½“çš„ãªåˆ†é‡åã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ±ºå®š
    dir_path = None
    field_detail = field_raw

    for part in reversed(parts):
        part_clean = part.strip()
        if part_clean in FIELD_TO_DIR:
            dir_path = FIELD_TO_DIR[part_clean]
            break

    # å¤§åˆ†é¡ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not dir_path and len(parts) >= 1:
        top = parts[0].strip()
        if "ãƒ†ã‚¯ãƒãƒ­ã‚¸" in top:
            dir_path = "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ "
        elif "ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ" in top:
            dir_path = "ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆç³»"
        elif "ã‚¹ãƒˆãƒ©ãƒ†ã‚¸" in top:
            dir_path = "ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ç³»"
        else:
            dir_path = "ãƒ†ã‚¯ãƒãƒ­ã‚¸ç³»/æŠ€è¡“è¦ç´ "

    return dir_path, field_detail


# â”€â”€ ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_question_id(year_code, q_number):
    """å•é¡ŒIDã‚’ç”Ÿæˆã™ã‚‹"""
    # R05 â†’ R05, H30æ˜¥ â†’ H30H, H30ç§‹ â†’ H30A
    base = year_code.replace("æ˜¥", "H").replace("ç§‹", "A").replace("ç‰¹", "T")
    return f"{base}_KA_Q{q_number:02d}"


def generate_markdown(data, year_code, q_number):
    """å•é¡Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç”Ÿæˆã™ã‚‹"""
    q_id = generate_question_id(year_code, q_number)
    dir_path, field_detail = parse_field(data.get("field_raw", ""))

    # è¥¿æš¦å¹´
    year_base = re.match(r'[RH]\d+', year_code)
    western_year = YEAR_TO_WESTERN.get(year_base.group() if year_base else year_code, 0)

    # å•é¡Œã‚¿ã‚¤ãƒ—è‡ªå‹•åˆ¤å®š
    q_type = classify_question_type(
        data.get("question_text", ""),
        data.get("explanation_general", "") + data.get("explanation_choices", ""),
        dir_path
    )

    # æ—¥æœ¬èªãƒ©ãƒ™ãƒ«
    year_label = YEAR_TO_LABEL.get(year_code, year_code)

    # é¸æŠè‚¢
    choices_text = ""
    for label in ["ã‚¢", "ã‚¤", "ã‚¦", "ã‚¨"]:
        if label in data.get("choices", {}):
            choices_text += f"- {label}: {data['choices'][label]}\n"

    # è§£èª¬çµ„ã¿ç«‹ã¦
    explanation = ""
    if data.get("explanation_general"):
        explanation += data["explanation_general"] + "\n\n"
    if data.get("explanation_choices"):
        explanation += "### å„é¸æŠè‚¢ã®è§£èª¬\n" + data["explanation_choices"]

    # é–¢é€£æ¦‚å¿µã®æŠ½å‡º
    related = extract_related_concepts(data)
    related_text = "\n".join(f"- [[{c}]]" for c in related)

    md = f"""---
id: {q_id}
year: {year_code}
kamoku: A
number: {q_number}
type: {q_type}
field: {field_detail}
difficulty: 3
status: æœªå­¦ç¿’
source: "å‡ºå…¸ï¼š{year_label} åŸºæœ¬æƒ…å ±æŠ€è¡“è€…è©¦é¨“ ç§‘ç›®A å•{q_number}"
source_url: "{data.get('source_url', '')}"
last_attempt: null
confidence_history: []
result_history: []
time_spent_seconds: []
---

## å•é¡Œ
{data.get('question_text', 'ï¼ˆå–å¾—å¤±æ•—ï¼‰')}

## é¸æŠè‚¢
{choices_text.rstrip()}

## æ­£è§£
{data.get('answer', '?')}

## è§£èª¬
{explanation.rstrip()}

### é–¢é€£æ¦‚å¿µ
{related_text}
"""
    return md.strip() + "\n", dir_path, q_id


def extract_related_concepts(data):
    """å•é¡Œæ–‡ã¨è§£èª¬ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹"""
    text = (data.get("question_text", "") + " " +
            data.get("explanation_general", "") + " " +
            data.get("explanation_choices", ""))

    # ã‚ˆãå‡ºã‚‹æŠ€è¡“ç”¨èªãƒ‘ã‚¿ãƒ¼ãƒ³
    concepts = set()
    patterns = [
        r'[A-Z]{2,}[/ãƒ»][A-Z]+',  # SSL/TLS, TCP/IP
        r'[A-Z]{2,}(?:\s|$)',     # HTTP, SQL, etc.
    ]

    for pattern in patterns:
        for match in re.findall(pattern, text):
            m = match.strip()
            if len(m) >= 2 and m not in ("OR", "AND", "NOT", "ON", "IF", "IN", "AS", "BY"):
                concepts.add(m)

    # æ—¥æœ¬èªã®æŠ€è¡“ç”¨èª
    jp_terms = [
        "æš—å·åŒ–", "å¾©å·", "èªè¨¼", "ç½²å", "ãƒãƒƒã‚·ãƒ¥", "å…¬é–‹éµ", "ç§˜å¯†éµ", "å…±é€šéµ",
        "ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«", "æ­£è¦åŒ–", "ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³", "ç¨¼åƒç‡", "å†—é•·",
        "ãƒ—ãƒ­ãƒˆã‚³ãƒ«", "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘", "ãƒ†ã‚¹ãƒˆ", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ç›£æŸ»",
    ]
    for term in jp_terms:
        if term in text:
            concepts.add(term)

    return sorted(list(concepts))[:5]  # æœ€å¤§5å€‹


# â”€â”€ ä¿å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def save_question(md_content, dir_path, q_id):
    """å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹"""
    target_dir = QUESTIONS_DIR / dir_path
    target_dir.mkdir(parents=True, exist_ok=True)

    filename = f"{q_id}.md"
    filepath = target_dir / filename

    if filepath.exists():
        return filepath, True  # æ—¢ã«å­˜åœ¨ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰

    filepath.write_text(md_content, encoding="utf-8")
    return filepath, False


# â”€â”€ learning_progress.md æ›´æ–° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def count_total_questions():
    """questions/ é…ä¸‹ã®ç·å•é¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹"""
    count = 0
    for md_file in QUESTIONS_DIR.rglob("*.md"):
        if md_file.name != "_ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ.md":
            count += 1
    return count


# â”€â”€ ãƒ¡ã‚¤ãƒ³å‡¦ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def resolve_year_url(year_code):
    """å¹´åº¦ã‚³ãƒ¼ãƒ‰ã‹ã‚‰URLãƒ‘ã‚¹ã‚’è§£æ±ºã™ã‚‹"""
    if year_code in YEAR_MAP:
        return YEAR_MAP[year_code]

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è‡ªå‹•æ¨å®š
    # R05 â†’ 05_menjo, H30æ˜¥ â†’ 30_haru
    match = re.match(r'([RH])(\d+)(æ˜¥|ç§‹|ç‰¹)?', year_code)
    if not match:
        return None

    era, num, season = match.groups()
    num = int(num)

    if era == "R" and num >= 2:
        return f"{num:02d}_menjo"
    elif season == "æ˜¥":
        return f"{num:02d}_haru"
    elif season == "ç§‹":
        return f"{num:02d}_aki"
    elif season == "ç‰¹":
        return f"{num:02d}_toku"
    else:
        # å¹³æˆã§å­£ç¯€æŒ‡å®šãªã— â†’ æ˜¥ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return f"{num:02d}_haru"


def fetch_single_question(year_code, q_number, verbose=True):
    """1å•ã‚’å–å¾—ã—ã¦ä¿å­˜ã™ã‚‹"""
    url_path = resolve_year_url(year_code)
    if not url_path:
        print(f"  âŒ å¹´åº¦ã‚³ãƒ¼ãƒ‰ '{year_code}' ã‚’è§£æ±ºã§ãã¾ã›ã‚“")
        return False

    url = f"{BASE_URL}/{url_path}/q{q_number}.html"

    if verbose:
        print(f"  ğŸ“¥ å–å¾—ä¸­: {url}")

    soup, err = fetch_question_page(url)
    if err:
        print(f"  âŒ å–å¾—å¤±æ•—: {err}")
        return False

    data, err = parse_question(soup, url)
    if err:
        print(f"  âŒ ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {err}")
        return False

    md_content, dir_path, q_id = generate_markdown(data, year_code, q_number)
    filepath, skipped = save_question(md_content, dir_path, q_id)

    if skipped:
        if verbose:
            print(f"  â­ï¸  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ï¼‰: {filepath.relative_to(REPO_ROOT)}")
        return True

    if verbose:
        print(f"  âœ… ä¿å­˜: {filepath.relative_to(REPO_ROOT)}")
        print(f"     åˆ†é‡: {data.get('field_raw', 'ä¸æ˜')}")
        print(f"     æ­£è§£: {data.get('answer', '?')}")

    return True


def fetch_range(year_code, start, end, verbose=True):
    """ç¯„å›²æŒ‡å®šã§å•é¡Œã‚’å–å¾—ã™ã‚‹"""
    total = min(end - start + 1, MAX_QUESTIONS_PER_RUN)
    success = 0
    fail = 0
    skip = 0

    print(f"\nğŸ”„ {YEAR_TO_LABEL.get(year_code, year_code)} å•{start}ã€œå•{start + total - 1} ã‚’å–å¾—ã—ã¾ã™...\n")

    for i, q_num in enumerate(range(start, start + total)):
        if i > 0:
            time.sleep(REQUEST_INTERVAL)

        url_path = resolve_year_url(year_code)
        url = f"{BASE_URL}/{url_path}/q{q_num}.html"

        print(f"[{i+1}/{total}] å•{q_num}: ", end="", flush=True)

        soup, err = fetch_question_page(url)
        if err:
            print(f"âŒ {err}")
            fail += 1
            continue

        data, err = parse_question(soup, url)
        if err:
            print(f"âŒ {err}")
            fail += 1
            continue

        md_content, dir_path, q_id = generate_markdown(data, year_code, q_num)
        filepath, skipped = save_question(md_content, dir_path, q_id)

        if skipped:
            print(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ï¼‰")
            skip += 1
        else:
            field = data.get('field_raw', 'ä¸æ˜')
            answer = data.get('answer', '?')
            print(f"âœ… {q_id} [{field}] æ­£è§£:{answer}")
            success += 1

    # ã‚µãƒãƒªãƒ¼
    total_questions = count_total_questions()
    print(f"\n{'â”€' * 50}")
    print(f"ğŸ“Š çµæœã‚µãƒãƒªãƒ¼")
    print(f"   æ–°è¦å–å¾—: {success}å•")
    print(f"   ã‚¹ã‚­ãƒƒãƒ—: {skip}å•")
    print(f"   å¤±æ•—:     {fail}å•")
    print(f"   å•é¡Œãƒãƒ³ã‚¯ç·æ•°: {total_questions}å•")
    print(f"{'â”€' * 50}")

    return success


def main():
    if len(sys.argv) < 3:
        print("ä½¿ã„æ–¹:")
        print("  python scripts/fetch_question.py <å¹´åº¦> <å•ç•ªå·|ç¯„å›²|all>")
        print()
        print("MAX_QUESTIONS_PER_RUN: 80ï¼ˆ1å›ã§æœ€å¤§80å•ï¼‰")
        print("allæŒ‡å®š: R02ä»¥é™=60å•ã€R01ä»¥å‰=80å•")
        print()
        print("ä¾‹:")
        print("  python scripts/fetch_question.py R05 15        # 1å•å–å¾—")
        print("  python scripts/fetch_question.py R05 1-20      # ç¯„å›²å–å¾—")
        print("  python scripts/fetch_question.py R05 all       # å…¨å•å–å¾—ï¼ˆR02ä»¥é™:60å•, R01ä»¥å‰:80å•ï¼‰")
        print("  python scripts/fetch_question.py H30æ˜¥ 32      # æ—§åˆ¶åº¦")
        print()
        print("å¯¾å¿œå¹´åº¦:")
        for code in sorted(YEAR_MAP.keys()):
            label = YEAR_TO_LABEL.get(code, code)
            print(f"  {code:8s} â†’ {label}")
        sys.exit(1)

    year_code = sys.argv[1]
    target = sys.argv[2]

    # URLãƒ‘ã‚¹ç¢ºèª
    url_path = resolve_year_url(year_code)
    if not url_path:
        print(f"âŒ å¹´åº¦ã‚³ãƒ¼ãƒ‰ '{year_code}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        print(f"   å¯¾å¿œå¹´åº¦ä¸€è¦§ã¯å¼•æ•°ãªã—ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    if target == "all":
        # æ–°åˆ¶åº¦(R02ã€œ): 60å•ã€æ—§åˆ¶åº¦(ã€œR01): 80å•
        year_base = re.match(r'([RH])(\d+)', year_code)
        if year_base:
            era, num = year_base.group(1), int(year_base.group(2))
            if era == "R" and num >= 2:
                max_q = 60
            else:
                max_q = 80
        else:
            max_q = 80
        fetch_range(year_code, 1, max_q)
    elif "-" in target:
        # ç¯„å›²æŒ‡å®š
        parts = target.split("-")
        start = int(parts[0])
        end = int(parts[1])
        fetch_range(year_code, start, end)
    else:
        # å˜ä¸€å•é¡Œ
        q_number = int(target)
        print(f"\nğŸ”„ {YEAR_TO_LABEL.get(year_code, year_code)} å•{q_number} ã‚’å–å¾—ã—ã¾ã™...\n")
        success = fetch_single_question(year_code, q_number)
        if success:
            total = count_total_questions()
            print(f"\n   å•é¡Œãƒãƒ³ã‚¯ç·æ•°: {total}å•")


if __name__ == "__main__":
    main()
